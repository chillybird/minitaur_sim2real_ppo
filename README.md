# minitaur_sim2real_ppo
环境代码来自于[bullet3](https://github.com/chillybird/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/minitaur)，这里进行了一些修改，使得更容易使用，并使用 pytorch 版本的[PPO算法](https://github.com/chillybird/PPO-PyTorch)进行了训练
