# minitaur_sim2real_ppo
环境代码来自于https://github.com/chillybird/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/minitaur，这里进行了一些修改，使得更容易使用，并使用 pytorch 版本的PPO算法（https://github.com/chillybird/PPO-PyTorch）进行了训练
